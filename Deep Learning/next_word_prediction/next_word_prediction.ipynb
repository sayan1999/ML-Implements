{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation marks from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_spl_char(sent):\n",
    "    \n",
    "    punctuation = re.compile('[^a-zA-Z\\-]*')\n",
    "    post_punctutation = []\n",
    "    for word in sent:\n",
    "        word=punctuation.sub(\"\", word.lower())\n",
    "        if len(word) > 0 and word != 's':\n",
    "            post_punctutation.append(word)\n",
    "\n",
    "    return post_punctutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus\n",
    "list_of_sent = []\n",
    "for name in corpus.gutenberg.fileids():\n",
    "    sublist_of_sent = corpus.gutenberg.sents(name)\n",
    "    list_of_sent.extend(sublist_of_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tokenized_sent = [remove_spl_char(sent) for sent in list_of_sent]\n",
    "# list_of_tokenized_sent\n",
    "for elem in list_of_tokenized_sent:\n",
    "    if len(elem) <= 3:\n",
    "        list_of_tokenized_sent.remove(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "ml=gensim.models.Word2Vec(list_of_tokenized_sent, size=50, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input and target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "word_input = [ [ ml.wv[sent[i]], ml.wv[sent[i+1]], ml.wv[sent[i+2]] ] for sent in list_of_tokenized_sent for i in range(len(sent)-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_output = [ ml.wv[sent[i]] for sent in list_of_tokenized_sent for i in range(3, len(sent))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(word_input), np.array(word_output), test_size=0.2, random_state =22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM((350), activation = 'relu', batch_input_shape=(None, 3, 50), return_sequences = True))\n",
    "model.add(LSTM((150), activation = 'relu', return_sequences = True))\n",
    "model.add(LSTM((50), activation = 'relu', return_sequences = False))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 5, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=dict(zip(ml.wv.vectors[0], ml.wv.index2word))\n",
    "dictionary[ml['the'][0]]\n",
    "y_test[:, 0]\n",
    "model.predict(x_test)[:, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
